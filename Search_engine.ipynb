{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4111b512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import joblib\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_sqlalchemy import SQLAlchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f221b5",
   "metadata": {},
   "source": [
    "## Preprocessor Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c6d126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def get_wordnet_pos(self, tag):\n",
    "        tag_dict = {\n",
    "            \"J\": wordnet.ADJ,\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"R\": wordnet.ADV\n",
    "        }\n",
    "        return tag_dict.get(tag[0].upper(), wordnet.NOUN)\n",
    "\n",
    "    def lemmatization(self, tagged_doc_text):\n",
    "        return [self.lemmatizer.lemmatize(word, pos=self.get_wordnet_pos(tag)) for word, tag in tagged_doc_text]\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tokens = [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens if token.translate(str.maketrans('', '', string.punctuation))]\n",
    "        tokens = [word for word in tokens if word not in self.stop_words]\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        tokens = self.lemmatization(tagged_tokens)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f08ad280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spell Corrector Service\n",
    "class SpellCorrector:\n",
    "    def __init__(self):\n",
    "        self.spell_checker = SpellChecker()\n",
    "\n",
    "    def correct_sentence_spelling(self, query):\n",
    "        query_tokens = word_tokenize(query.lower())\n",
    "        misspelled = self.spell_checker.unknown(query_tokens)\n",
    "        corrected_tokens = [self.spell_checker.correction(token) if token in misspelled else token for token in query_tokens]\n",
    "        return ' '.join(corrected_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9427b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer Service\n",
    "class Vectorizer:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def fit_transform(self, documents):\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(documents)\n",
    "        return self.tfidf_matrix\n",
    "\n",
    "    def transform(self, document):\n",
    "        return self.vectorizer.transform([document])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebcf2a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SearchEngine Service\n",
    "class SearchEngine:\n",
    "    def __init__(self, preprocessor, spell_corrector, document_model, app):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.spell_corrector = spell_corrector\n",
    "        self.vectorizers = {}\n",
    "        self.tfidf_matrices = {}\n",
    "        \n",
    "        with app.app_context():\n",
    "            self.documents = [doc.to_dict() for doc in document_model.query.all()]\n",
    "            self.elements = document_model.get_columns(True)\n",
    "\n",
    "    def index_documents(self):\n",
    "        for element in self.elements:\n",
    "            try:\n",
    "                processed_docs = [' '.join(self.preprocessor.preprocess(doc[element])) for doc in self.documents]\n",
    "                vectorizer = Vectorizer()\n",
    "                self.vectorizers[element] = vectorizer\n",
    "                self.tfidf_matrices[element] = vectorizer.fit_transform(processed_docs)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred during vectorization of {element}:\", e)\n",
    "    \n",
    "    def save_model(self, name):\n",
    "        joblib.dump(self.vectorizers, f'{name}_vectorizers.pkl')\n",
    "        joblib.dump(self.tfidf_matrices, f'{name}tfidf_matrices.pkl')\n",
    "\n",
    "    def load_model(self, name):\n",
    "        self.vectorizers = joblib.load(f'{name}_vectorizers.pkl')\n",
    "        self.tfidf_matrices = joblib.load(f'{name}tfidf_matrices.pkl')\n",
    "\n",
    "    def search(self, query, weights):\n",
    "        corrected_query = self.spell_corrector.correct_sentence_spelling(query)\n",
    "        query_processed = self.preprocessor.preprocess(corrected_query)\n",
    "        if not query_processed:\n",
    "            return [], [], corrected_query\n",
    "        \n",
    "        query = ' '.join(query_processed)\n",
    "        scores = np.zeros(len(self.documents))\n",
    "\n",
    "        for element, weight in zip(self.elements, weights):\n",
    "            try:\n",
    "                query_vector = self.vectorizers[element].transform(query)\n",
    "                if query_vector.shape[1] == 0:\n",
    "                    continue\n",
    "                cosine_similarities = cosine_similarity(query_vector, self.tfidf_matrices[element]).flatten()\n",
    "                scores += weight * cosine_similarities\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred during searching in {element}:\", e)\n",
    "\n",
    "        ranked_indices = np.argsort(scores)[::-1]\n",
    "        return ranked_indices, scores, corrected_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1643c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_value(value):\n",
    "    text = re.sub(r\"'''''(.*?)'''''\", r\"\\1\", value)\n",
    "    text = re.sub(r\"'''(.*?)'''\", r\"\\1\", text)\n",
    "    text = re.sub(r\"''(.*?)''\", r\"\\1\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2427d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "app.config['SQLALCHEMY_DATABASE_URI'] = 'mysql://root:@localhost/ir_search_engine'\n",
    "db = SQLAlchemy(app)\n",
    "\n",
    "class Document(db.Model):\n",
    "    id = db.Column(db.Integer, primary_key=True)\n",
    "    page_title = db.Column(db.String(255))\n",
    "    wikidata_classes = db.Column(db.String(255))\n",
    "    text = db.Column(db.Text)\n",
    "    sections = db.Column(db.Text)\n",
    "    infoboxes = db.Column(db.Text)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'page_title': self.page_title,\n",
    "            'wikidata_classes': self.wikidata_classes,\n",
    "            'text': self.text,\n",
    "            'sections': self.sections,\n",
    "            'infoboxes': self.infoboxes,\n",
    "        }\n",
    "        \n",
    "    def get_columns(self, exclude_id=False):\n",
    "        columns = self.__table__.columns\n",
    "        if exclude_id:\n",
    "            return [column.name for column in columns if column.name != 'id']\n",
    "        else:\n",
    "            return [column.name for column in columns]\n",
    "\n",
    "with app.app_context():\n",
    "    db.create_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d5ebfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_document(doc_data):\n",
    "    doc = Document(\n",
    "        page_title=doc_data['page_title'],\n",
    "        wikidata_classes=doc_data['wikidata_classes'],\n",
    "        text=doc_data['text'],\n",
    "        sections=doc_data['sections'],\n",
    "        infoboxes=doc_data['infoboxes']\n",
    "    )\n",
    "    db.session.add(doc)\n",
    "    db.session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8e16626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Application\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = ir_datasets.load(\"trec-tot/2023/train\")\n",
    "\n",
    "    for i, doc in enumerate(dataset.docs_iter()):\n",
    "        # if i == 1000:\n",
    "        #     break\n",
    "        doc_data = {\n",
    "            \"page_title\": doc.page_title,\n",
    "            \"wikidata_classes\": doc.wikidata_classes[0][1],\n",
    "            \"text\": doc.text,\n",
    "            \"sections\": \"\",\n",
    "            \"infoboxes\": \"\",\n",
    "        }\n",
    "    \n",
    "        for section_name, section_text in doc.sections.items():\n",
    "            doc_data[\"sections\"] += f\"\\n {section_text}\"\n",
    "    \n",
    "        for infobox in doc.infoboxes:\n",
    "            for key, value in infobox['params'].items():\n",
    "                cleaned_value = clean_value(value)\n",
    "                doc_data[\"infoboxes\"] += f\"\\n {cleaned_value}\"\n",
    "        with app.app_context():\n",
    "            save_document(doc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15d3362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_model = Document()\n",
    "preprocessor = Preprocessor()\n",
    "spell_corrector = SpellCorrector()\n",
    "search_engine = SearchEngine(preprocessor, spell_corrector, document_model, app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465e4815",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_engine.index_documents()\n",
    "search_engine.save_model(\"trec-tot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f6dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"film television Chldren\"\n",
    "weights = [0.5, 1.5, 0.3, 0.1, 0.1]\n",
    "search_engine.load_model(\"trec-tot\")\n",
    "ranked_indices, scores, corrected_query = search_engine.search(query, weights)\n",
    "if corrected_query != query.lower():\n",
    "        print(\"Did you mean: \" + str(corrected_query))\n",
    "\n",
    "print(\"\\n\\nRanked Documents:\\n\")\n",
    "for index in ranked_indices[:10]:\n",
    "    print(f\"Document {index} - Score: {scores[index]:.4f}\")\n",
    "    print(f\"Title: {corpus[index]['page_title']}\")\n",
    "    print(f\"Classes: {corpus[index]['wikidata_classes']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "067db8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/search', methods=['GET'])\n",
    "def search():\n",
    "    query = request.args.get('query', '')\n",
    "    type_dataset = request.args.get('type_dataset', '')\n",
    "\n",
    "    if not query:\n",
    "        return jsonify({\n",
    "            'message': 'Query parameter is required'\n",
    "        }), 400\n",
    "        \n",
    "    if not type_dataset:\n",
    "        return jsonify({\n",
    "            'message': 'Type dataset parameter is required'\n",
    "        }), 400\n",
    "\n",
    "    if type_dataset == \"1\":\n",
    "        search_engine.load_model(\"trec-tot\")\n",
    "    elif type_dataset == \"2\":\n",
    "        search_engine.load_model(\"webis-touche2020\")\n",
    "    else:\n",
    "        return jsonify({\n",
    "            'message': \"Type dataset not valid\"\n",
    "        }), 400\n",
    "        \n",
    "    ranked_indices, scores, corrected_query = search_engine.search(query, weights)\n",
    "    \n",
    "    results = []\n",
    "    for index in ranked_indices[:10]:\n",
    "        doc = search_engine.documents[index]\n",
    "        results.append({\n",
    "            'title': doc['page_title'],\n",
    "            'wikidata_classes': doc['wikidata_classes'],\n",
    "            'text_snippet': ' '.join(doc['text'].split()[:30]),\n",
    "            'similarity_score': f'{scores[index]:.4f}'\n",
    "        })\n",
    "\n",
    "    response = {\n",
    "        'corrected_query': corrected_query if corrected_query.lower() != query.lower() else None,\n",
    "        'results': results\n",
    "    }\n",
    "    \n",
    "    return jsonify(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32651b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(debug=False)\n",
    "    # %tb\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
