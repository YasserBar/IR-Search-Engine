{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02199d34-4ad8-4cf6-995e-da700360872f",
   "metadata": {},
   "source": [
    "# Notebook: Search Engine with Python, NLTK, and Scikit-learn\n",
    "# Author: [Yasser Barghouth]\n",
    "\n",
    "# Description: This notebook demonstrates the creation of a simple search engine using a service-oriented approach.\n",
    "#              It utilizes NLTK for text preprocessing and Scikit-learn for vectorization and search functionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad8eaf0-a3e0-4644-b180-36270e4627e8",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad25c6b3-c89e-46a5-83c8-1f850a878e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize , sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import string\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from tabulate import tabulate\n",
    "import mwparserfromhell\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import joblib\n",
    "import json\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_sqlalchemy import SQLAlchemy\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import Column, Integer, String, Float, Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3db4dc-d942-45b7-b1b0-59582a2726a2",
   "metadata": {},
   "source": [
    "# Preprocessor Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "752e6ff5-4bd5-4d83-9e3a-6b877c4e700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        # Initialize stopwords and stemmer\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def get_wordnet_pos(self, tag):\n",
    "        \"\"\"\n",
    "        Map POS tag to first character lemmatize() accepts.\n",
    "        \n",
    "        Args:\n",
    "            tag (str): The POS tag.\n",
    "        \n",
    "        Returns:\n",
    "            str: The corresponding wordnet POS tag.\n",
    "        \"\"\"\n",
    "        tag_dict = {\n",
    "            \"J\": wordnet.ADJ,\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"R\": wordnet.ADV\n",
    "        }\n",
    "        return tag_dict.get(tag[0].upper(), wordnet.NOUN)\n",
    "\n",
    "    def lemmatization(self, tagged_doc_text):\n",
    "        \"\"\"\n",
    "        Lemmatize the document text based on POS tags.\n",
    "        \n",
    "        Args:\n",
    "            tagged_doc_text (list): A list of tuples with word and POS tag.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of lemmatized words.\n",
    "        \"\"\"\n",
    "        return [self.lemmatizer.lemmatize(word, pos=self.get_wordnet_pos(tag)) for word, tag in tagged_doc_text]\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        \"\"\"\n",
    "        Preprocesses the input text by tokenizing, removing stopwords, and lemmatizing.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text to preprocess.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of preprocessed tokens.\n",
    "        \"\"\"\n",
    "        # print(\"start processe ==================\")\n",
    "        \n",
    "        # Tokenize text\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        # print(\"Tokenize & lower text\")\n",
    "        # Remove punctuation tokens\n",
    "        tokens = [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens if token.translate(str.maketrans('', '', string.punctuation))]\n",
    "        # print(\"Remove punctuation tokens\")\n",
    "        # Remove stopwords\n",
    "        tokens = [word for word in tokens if word not in self.stop_words]\n",
    "        # print(\"Remove stopwords\")\n",
    "        # Apply Part-of-speech Tagging\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        # print(\"Apply Part-of-speech Tagging\")\n",
    "        # print(\"Part-of-speech Tagging tokens : \" + str(tagged_tokens[:5]))\n",
    "        # Apply lemmatization\n",
    "        tokens = self.lemmatization(tagged_tokens)\n",
    "        # print(\"Apply lemmatization\")\n",
    "        # print(\"lemmatized tokens : \" + str(tokens[:5]))\n",
    "\n",
    "        # print(\"end processe ==================\")\n",
    "        \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465acf68-000f-4444-9964-9518366245a2",
   "metadata": {},
   "source": [
    "## Spell Corrector Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f2deb5b-b0b1-447d-84b1-37fed2adfc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellCorrector:\n",
    "    def __init__(self):\n",
    "        self.spell_checker = SpellChecker()\n",
    "\n",
    "    def correct_sentence_spelling(self, query):\n",
    "        \"\"\"\n",
    "        Corrects the spelling of query.\n",
    "        \n",
    "        Args:\n",
    "            query (str): A query text.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of corrected tokens.\n",
    "        \"\"\"\n",
    "        query_tokens = word_tokenize(query.lower())\n",
    "        misspelled = self.spell_checker.unknown(query_tokens)\n",
    "        corrected_tokens = [self.spell_checker.correction(token) if token in misspelled else token for token in query_tokens]\n",
    "        return ' '.join(corrected_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6377d54a-7c7b-4ded-b221-f194d0d599c7",
   "metadata": {},
   "source": [
    "# Vectorizer Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6e66521-8ec4-46a7-b876-a0d87bec16a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    def __init__(self):\n",
    "        # Initialize TF-IDF vectorizer\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def fit_transform(self, documents):\n",
    "        \"\"\"\n",
    "        Fits the vectorizer to the documents and transforms them into TF-IDF vectors.\n",
    "        \n",
    "        Args:\n",
    "            documents (list): A list of documents (strings).\n",
    "        \n",
    "        Returns:\n",
    "            sparse matrix: The document-term matrix.\n",
    "        \"\"\"\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(documents)\n",
    "        return self.tfidf_matrix\n",
    "\n",
    "    def transform(self, document):\n",
    "        \"\"\"\n",
    "        Transforms a single document into a TF-IDF vector.\n",
    "        \n",
    "        Args:\n",
    "            document (str): The document to transform.\n",
    "        \n",
    "        Returns:\n",
    "            sparse matrix: The TF-IDF vector of the document.\n",
    "        \"\"\"\n",
    "        return self.vectorizer.transform([document])\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        \"\"\"\n",
    "        Retrieves the feature names (terms) from the vectorizer.\n",
    "        \n",
    "        Returns:\n",
    "            array: An array of feature names.\n",
    "        \"\"\"\n",
    "        return self.vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5559a82-08c3-4395-8c18-4c6780b4fc49",
   "metadata": {},
   "source": [
    "# SearchEngine Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44d71549-e8f5-432b-9c46-65e4ed58a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngine:\n",
    "    def __init__(self, preprocessor, spell_corrector, document_model, app):\n",
    "        # Initialize with preprocessor, spell corrector and vectorizer and matrice services\n",
    "        self.preprocessor = preprocessor\n",
    "        self.spell_corrector = spell_corrector\n",
    "        self.vectorizers = {}\n",
    "        self.tfidf_matrices = {}\n",
    "        \n",
    "        with app.app_context():\n",
    "            self.documents = [doc.to_dict() for doc in document_model.query.all()]\n",
    "            self.elements = document_model.get_columns(True)\n",
    "\n",
    "    def index_documents(self):\n",
    "        \"\"\"\n",
    "        Indexes the documents by preprocessing and vectorizing them.\n",
    "        \n",
    "        Args:\n",
    "            documents (list): A list of documents to index.\n",
    "        \"\"\"\n",
    "        for element in self.elements:\n",
    "            try:\n",
    "                # print(\"processed docs elment \" + element + \" :\")\n",
    "                # print(\"before processe *******************************************\")\n",
    "                # Preprocess documents\n",
    "                processed_docs = [' '.join(self.preprocessor.preprocess(doc[element])) for doc in self.documents]\n",
    "                # print(\"after processe *******************************************\")\n",
    "                \n",
    "                vectorizer = Vectorizer()\n",
    "                self.vectorizers[element] = vectorizer\n",
    "                # Vectorize documents\n",
    "                self.tfidf_matrices[element] = vectorizer.fit_transform(processed_docs)\n",
    "                # print(f\"Vectorization successful for {element}. Matrix shape:\", self.tfidf_matrices[element].shape)\n",
    "                # print(\"******************************************************************\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred during vectorization of {element}:\", e)\n",
    "    \n",
    "    def save_model(self, name):\n",
    "        joblib.dump(self.vectorizers, f'{name}_vectorizers.pkl')\n",
    "        joblib.dump(self.tfidf_matrices, f'{name}tfidf_matrices.pkl')\n",
    "\n",
    "    def load_model(self,name):\n",
    "        self.vectorizers = joblib.load(f'{name}_vectorizers.pkl')\n",
    "        self.tfidf_matrices = joblib.load(f'{name}tfidf_matrices.pkl')\n",
    "\n",
    "    def search(self, query, weights):\n",
    "        \"\"\"\n",
    "        Searches the indexed documents for the given query and ranks them by relevance.\n",
    "        \n",
    "        Args:\n",
    "            query (str): The search query.\n",
    "            elements (list): List of elements to search within.\n",
    "            weights (list): List of weights for each element.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Ranked indices of documents, their similarity scores and corrected query.\n",
    "        \"\"\"\n",
    "        \n",
    "        # print(\"query : \" + query)\n",
    "\n",
    "        # Correct spelling of the query\n",
    "        corrected_query = self.spell_corrector.correct_sentence_spelling(query)\n",
    "        # print(\"corrected_query : \" + str(corrected_query))\n",
    "        \n",
    "        # Preprocess the query\n",
    "        query_processed = self.preprocessor.preprocess(corrected_query)\n",
    "        # print(\"query_processed : \" + str(query_processed))\n",
    "        \n",
    "        if not query_processed:\n",
    "            print(\"Query processed to an empty list. Cannot search with an empty query.\")\n",
    "            return [], [], corrected_query\n",
    "        \n",
    "        query = ' '.join(query_processed)\n",
    "        # print(\"query joined : \" + query)\n",
    "        \n",
    "        scores = np.zeros(len(self.documents))\n",
    "\n",
    "        for element, weight in zip(self.elements, weights):\n",
    "            try:\n",
    "                query_vector = self.vectorizers[element].transform(query)\n",
    "                \n",
    "                if query_vector.shape[1] == 0:\n",
    "                    print(f\"No valid terms in query for element {element}. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                # Compute cosine similarities between the query and the documents\n",
    "                cosine_similarities = cosine_similarity(query_vector, self.tfidf_matrices[element]).flatten()\n",
    "                scores += weight * cosine_similarities\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred during searching in {element}:\", e)\n",
    "\n",
    "        # Rank documents by similarity\n",
    "        ranked_indices = np.argsort(scores)[::-1]\n",
    "        return ranked_indices, scores, corrected_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2dbd1a4-f50f-4ea6-8b4b-5a85c9ea0312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_value(value):\n",
    "    # Use mwparserfromhell to parse the value\n",
    "    wikicode = mwparserfromhell.parse(value)\n",
    "    text = wikicode.strip_code()  # Convert to plain text\n",
    "\n",
    "    # Remove Markdown-like syntax for bold/italic\n",
    "    text = re.sub(r\"'''''(.*?)'''''\", r\"\\1\", text)  # Bold and italic\n",
    "    text = re.sub(r\"'''(.*?)'''\", r\"\\1\", text)      # Bold\n",
    "    text = re.sub(r\"''(.*?)''\", r\"\\1\", text)        # Italic\n",
    "\n",
    "    # Normalize whitespaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03ed7d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "app = Flask(__name__)\n",
    "app.config['SQLALCHEMY_DATABASE_URI'] = 'mysql://root:@localhost/ir_search_engine'\n",
    "db = SQLAlchemy(app)\n",
    "\n",
    "class Document(db.Model):\n",
    "    id = db.Column(db.Integer, primary_key=True)\n",
    "    page_title = db.Column(db.String(255))\n",
    "    wikidata_classes = db.Column(db.String(255))\n",
    "    text = db.Column(db.Text)\n",
    "    sections = db.Column(db.Text)\n",
    "    infoboxes = db.Column(db.Text)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'page_title': self.page_title,\n",
    "            'wikidata_classes': self.wikidata_classes,\n",
    "            'text': self.text,\n",
    "            'sections': self.sections,\n",
    "            'infoboxes': self.infoboxes,\n",
    "        }\n",
    "        \n",
    "    def get_columns(self, exclude_id = False):\n",
    "        columns = self.__table__.columns\n",
    "        if exclude_id:\n",
    "            return [column.name for column in columns if column.name != 'id']\n",
    "        else:\n",
    "            return [column.name for column in columns]\n",
    "\n",
    "with app.app_context():\n",
    "    db.create_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be174f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_document(doc_data):\n",
    "    doc = Document(\n",
    "        page_title=doc_data['page_title'],\n",
    "        wikidata_classes=doc_data['wikidata_classes'],\n",
    "        text=doc_data['text'],\n",
    "        sections=doc_data['sections'],\n",
    "        infoboxes=doc_data['infoboxes']\n",
    "    )\n",
    "    db.session.add(doc)\n",
    "    db.session.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea57f0a-6dfc-43c1-b87b-f30df0b7ba98",
   "metadata": {},
   "source": [
    "# Main Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c437d359-b27b-4a9f-a2dd-b767de284f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Application\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    dataset = ir_datasets.load(\"trec-tot/2023/train\") # number of docs in this dataset is 231852 doc\n",
    "    \n",
    "    corpus = []\n",
    "\n",
    "    for i, doc in enumerate(dataset.docs_iter()):\n",
    "        if i == 1000:\n",
    "            break\n",
    "        \n",
    "        doc_data = {\n",
    "            \"page_title\": doc.page_title,\n",
    "            \"wikidata_classes\": doc.wikidata_classes[0][1],\n",
    "            \"text\": doc.text,\n",
    "            \"sections\": \"\",\n",
    "            \"infoboxes\": \"\",\n",
    "        }\n",
    "    \n",
    "        for section_name, section_text in doc.sections.items():\n",
    "            doc_data[\"sections\"] += f\"\\n {section_text}\"\n",
    "    \n",
    "        for infobox in doc.infoboxes:\n",
    "            for key, value in infobox['params'].items():\n",
    "                cleaned_value = clean_value(value)\n",
    "                doc_data[\"infoboxes\"] += f\"\\n {cleaned_value}\"\n",
    "        \n",
    "        corpus.append(doc_data)\n",
    "        with app.app_context():\n",
    "            save_document(doc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f30988b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save processed data\n",
    "# with open('cleaned_corpus.json', 'w') as f:\n",
    "#     json.dump(corpus, f)\n",
    "\n",
    "# # Load processed data\n",
    "# with open('cleaned_corpus.json', 'r') as f:\n",
    "#     corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa51f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Initialize services\n",
    "    document_model = Document()\n",
    "    preprocessor = Preprocessor()\n",
    "    spell_corrector = SpellCorrector()\n",
    "    search_engine = SearchEngine(preprocessor, spell_corrector, document_model, app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a15466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # elements = [\"page_title\", \"wikidata_classes\", \"text\", \"sections\", \"infoboxes\"]\n",
    "    weights = [0.5, 1.5, 0.3, 0.1, 0.1]  # Assign weights based on importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0336a2cf-ed82-43af-a6e0-22141a120157",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Index documents\n",
    "    search_engine.index_documents()\n",
    "   \n",
    "    # Save the model\n",
    "    search_engine.save_model(\"trec-tot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b49d02",
   "metadata": {},
   "source": [
    "## Diagram Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6949d2e-184d-44a2-8fc7-b59a02c0554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # page_title ======================================\n",
    "    tfidf_matrix = search_engine.tfidf_matrices['page_title']\n",
    "    feature_names = search_engine.vectorizers['page_title'].get_feature_names()\n",
    "    word_scores = dict(zip(feature_names, np.array(tfidf_matrix.sum(axis=0)).flatten()))    \n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_scores)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud of TF-IDF Scores page title')\n",
    "    plt.show()\n",
    "\n",
    "    # تحويل مصفوفة TF-IDF إلى DataFrame لاستخدامها مع seaborn\n",
    "    tfidf_df = pd.DataFrame(search_engine.tfidf_matrices['page_title'].toarray(), columns=search_engine.vectorizers['page_title'].get_feature_names())\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(tfidf_df.T, cmap='viridis', cbar=True)\n",
    "    plt.title('Heatmap of TF-IDF Scores page title')\n",
    "    plt.xlabel('Document')\n",
    "    plt.ylabel('Term')\n",
    "    plt.show()\n",
    "\n",
    "    # Vectorize the corpus using one element, e.g., 'page_title'\n",
    "    tfidf_matrix = search_engine.tfidf_matrices['page_title'].toarray()\n",
    "\n",
    "    # Apply PCA to reduce to 3D\n",
    "    pca = PCA(n_components=3)\n",
    "    reduced_matrix = pca.fit_transform(tfidf_matrix)\n",
    "\n",
    "    # Plotting the results in 3D\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    ax.scatter(reduced_matrix[:, 0], reduced_matrix[:, 1], reduced_matrix[:, 2], c='blue', marker='o')\n",
    "\n",
    "    ax.set_title('3D Visualization of TF-IDF Matrix page title')\n",
    "    ax.set_xlabel('PCA Component 1')\n",
    "    ax.set_ylabel('PCA Component 2')\n",
    "    ax.set_zlabel('PCA Component 3')\n",
    "\n",
    "    doc_lengths = [len(doc[\"page_title\"].split()) for doc in corpus]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(doc_lengths, bins=30, kde=True)\n",
    "    plt.title('Distribution of Document\\'s page title Lengths')\n",
    "    plt.xlabel('Number of Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    # text ======================================\n",
    "    tfidf_matrix = search_engine.tfidf_matrices['text']\n",
    "    feature_names = search_engine.vectorizers['text'].get_feature_names()\n",
    "    word_scores = dict(zip(feature_names, np.array(tfidf_matrix.sum(axis=0)).flatten()))    \n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_scores)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud of TF-IDF Scores text')\n",
    "    plt.show()\n",
    "\n",
    "    # # تحويل مصفوفة TF-IDF إلى DataFrame لاستخدامها مع seaborn\n",
    "    # tfidf_df = pd.DataFrame(search_engine.tfidf_matrices['text'].toarray(), columns=search_engine.vectorizers['text'].get_feature_names())\n",
    "    \n",
    "    # plt.figure(figsize=(12, 100))\n",
    "    # sns.heatmap(tfidf_df.T, cmap='viridis', cbar=True)\n",
    "    # plt.title('Heatmap of TF-IDF Scores text')\n",
    "    # plt.xlabel('Document')\n",
    "    # plt.ylabel('Term')\n",
    "    # plt.show()\n",
    "    \n",
    "    # # Vectorize the corpus using one element, e.g., 'text'\n",
    "    # tfidf_matrix = search_engine.tfidf_matrices['text'].toarray()\n",
    "\n",
    "    # # Apply PCA to reduce to 3D\n",
    "    # pca = PCA(n_components=3)\n",
    "    # reduced_matrix = pca.fit_transform(tfidf_matrix)\n",
    "\n",
    "    # # Plotting the results in 3D\n",
    "    # fig = plt.figure(figsize=(10, 8))\n",
    "    # ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # ax.scatter(reduced_matrix[:, 0], reduced_matrix[:, 1], reduced_matrix[:, 2], c='blue', marker='o')\n",
    "\n",
    "    # ax.set_title('3D Visualization of TF-IDF Matrix text')\n",
    "    # ax.set_xlabel('PCA Component 1')\n",
    "    # ax.set_ylabel('PCA Component 2')\n",
    "    # ax.set_zlabel('PCA Component 3')\n",
    "\n",
    "    doc_lengths = [len(doc[\"text\"].split()) for doc in corpus]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(doc_lengths, bins=30, kde=True)\n",
    "    plt.title('Distribution of Document\\'s text Lengths')\n",
    "    plt.xlabel('Number of Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    # sections ======================================\n",
    "    tfidf_matrix = search_engine.tfidf_matrices['sections']\n",
    "    feature_names = search_engine.vectorizers['sections'].get_feature_names()\n",
    "    word_scores = dict(zip(feature_names, np.array(tfidf_matrix.sum(axis=0)).flatten()))    \n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_scores)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud of TF-IDF sections')\n",
    "    plt.show()\n",
    "\n",
    "    # # تحويل مصفوفة TF-IDF إلى DataFrame لاستخدامها مع seaborn\n",
    "    # tfidf_df = pd.DataFrame(search_engine.tfidf_matrices['sections'].toarray(), columns=search_engine.vectorizers['sections'].get_feature_names())\n",
    "    \n",
    "    # plt.figure(figsize=(12, 100))\n",
    "    # sns.heatmap(tfidf_df.T, cmap='viridis', cbar=True)\n",
    "    # plt.title('Heatmap of TF-IDF Scores sections')\n",
    "    # plt.xlabel('Document')\n",
    "    # plt.ylabel('Term')\n",
    "    # plt.show()\n",
    "    \n",
    "    # # Vectorize the corpus using one element, e.g., 'sections'\n",
    "    # tfidf_matrix = search_engine.tfidf_matrices['sections'].toarray()\n",
    "\n",
    "    # # Apply PCA to reduce to 3D\n",
    "    # pca = PCA(n_components=3)\n",
    "    # reduced_matrix = pca.fit_transform(tfidf_matrix)\n",
    "\n",
    "    # # Plotting the results in 3D\n",
    "    # fig = plt.figure(figsize=(10, 8))\n",
    "    # ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # ax.scatter(reduced_matrix[:, 0], reduced_matrix[:, 1], reduced_matrix[:, 2], c='blue', marker='o')\n",
    "\n",
    "    # ax.set_title('3D Visualization of TF-IDF Matrix sections')\n",
    "    # ax.set_xlabel('PCA Component 1')\n",
    "    # ax.set_ylabel('PCA Component 2')\n",
    "    # ax.set_zlabel('PCA Component 3')\n",
    "\n",
    "    doc_lengths = [len(doc[\"sections\"].split()) for doc in corpus]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(doc_lengths, bins=30, kde=True)\n",
    "    plt.title('Distribution of Document\\'s sections Lengths')\n",
    "    plt.xlabel('Number of Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    # infoboxes ======================================\n",
    "    tfidf_matrix = search_engine.tfidf_matrices['infoboxes']\n",
    "    feature_names = search_engine.vectorizers['infoboxes'].get_feature_names()\n",
    "    word_scores = dict(zip(feature_names, np.array(tfidf_matrix.sum(axis=0)).flatten()))    \n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_scores)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud of TF-IDF Scores infoboxes')\n",
    "    plt.show()\n",
    "\n",
    "    # # تحويل مصفوفة TF-IDF إلى DataFrame لاستخدامها مع seaborn\n",
    "    # tfidf_df = pd.DataFrame(search_engine.tfidf_matrices['infoboxes'].toarray(), columns=search_engine.vectorizers['infoboxes'].get_feature_names())\n",
    "    \n",
    "    # plt.figure(figsize=(12, 100))\n",
    "    # sns.heatmap(tfidf_df.T, cmap='viridis', cbar=True)\n",
    "    # plt.title('Heatmap of TF-IDF Scores infoboxes')\n",
    "    # plt.xlabel('Document')\n",
    "    # plt.ylabel('Term')\n",
    "    # plt.show()\n",
    "\n",
    "    # # Vectorize the corpus using one element, e.g., 'infoboxes'\n",
    "    # tfidf_matrix = search_engine.tfidf_matrices['infoboxes'].toarray()\n",
    "\n",
    "    # # Apply PCA to reduce to 3D\n",
    "    # pca = PCA(n_components=3)\n",
    "    # reduced_matrix = pca.fit_transform(tfidf_matrix)\n",
    "\n",
    "    # # Plotting the results in 3D\n",
    "    # fig = plt.figure(figsize=(10, 8))\n",
    "    # ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # ax.scatter(reduced_matrix[:, 0], reduced_matrix[:, 1], reduced_matrix[:, 2], c='blue', marker='o')\n",
    "\n",
    "    # ax.set_title('3D Visualization of TF-IDF Matrix infoboxes')\n",
    "    # ax.set_xlabel('PCA Component 1')\n",
    "    # ax.set_ylabel('PCA Component 2')\n",
    "    # ax.set_zlabel('PCA Component 3')\n",
    "\n",
    "    doc_lengths = [len(doc[\"infoboxes\"].split()) for doc in corpus]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(doc_lengths, bins=30, kde=True)\n",
    "    plt.title('Distribution of Document\\'s infoboxes Lengths')\n",
    "    plt.xlabel('Number of Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36910dca",
   "metadata": {},
   "source": [
    "## Query Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "86d4e058-5df1-4225-995c-da763e760ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did you mean: film television children\n",
      "\n",
      "\n",
      "Ranked Documents:\n",
      "\n",
      "Document 332 - Score: 1.3650\n",
      "Title: All My Children\n",
      "Classes: television program\n",
      "--------------------------------------------------\n",
      "Document 81 - Score: 1.2435\n",
      "Title: Married... with Children\n",
      "Classes: television program\n",
      "--------------------------------------------------\n",
      "Document 303 - Score: 1.1785\n",
      "Title: You Can't Do That on Television\n",
      "Classes: television program\n",
      "--------------------------------------------------\n",
      "Document 220 - Score: 1.1496\n",
      "Title: Marty (The Philco Television Playhouse)\n",
      "Classes: television program\n",
      "--------------------------------------------------\n",
      "Document 702 - Score: 1.1164\n",
      "Title: Where Are My Children?\n",
      "Classes: film\n",
      "--------------------------------------------------\n",
      "Document 263 - Score: 1.0153\n",
      "Title: Sesame Street\n",
      "Classes: television program\n",
      "--------------------------------------------------\n",
      "Document 106 - Score: 0.9937\n",
      "Title: Press Gang\n",
      "Classes: television program\n",
      "--------------------------------------------------\n",
      "Document 857 - Score: 0.9934\n",
      "Title: Knightmare\n",
      "Classes: television program\n",
      "--------------------------------------------------\n",
      "Document 889 - Score: 0.9881\n",
      "Title: Kids Incorporated\n",
      "Classes: television program\n",
      "--------------------------------------------------\n",
      "Document 170 - Score: 0.9835\n",
      "Title: Clangers\n",
      "Classes: television program\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "    # Example search query\n",
    "    query = \"film television Chldren\"\n",
    "    \n",
    "    search_engine.load_model(\"trec-tot\")\n",
    "    \n",
    "    # print(search_engine.)\n",
    "    \n",
    "    ranked_indices, scores, corrected_query = search_engine.search(query, weights)\n",
    "\n",
    "    # عرض التصحيحات للمستخدم\n",
    "    if corrected_query != query.lower() :\n",
    "        print(\"Did you mean: \" + str(corrected_query))\n",
    "        \n",
    "    # Display results\n",
    "    print(\"\\n\\nRanked Documents:\\n\")\n",
    "    for index in ranked_indices[:10]:  # Limit to top 10 results\n",
    "        print(f\"Document {index} - Score: {scores[index]:.4f}\")\n",
    "        print(f\"Title: {corpus[index]['page_title']}\")\n",
    "        print(f\"Classes: {corpus[index]['wikidata_classes']}\")\n",
    "        # Uncomment to display more details\n",
    "        # print(f\"Text: {corpus[index]['text']}\")\n",
    "        # print(f\"Sections: {corpus[index]['sections']}\")\n",
    "        # print(f\"Infoboxes: {corpus[index]['infoboxes']}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "00c6a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/search', methods=['GET'])\n",
    "def search():\n",
    "    query = request.args.get('query', '')\n",
    "    type_dataset = request.args.get('type_dataset', '')\n",
    "\n",
    "    if not query:\n",
    "        return jsonify({\n",
    "            'message': 'Query parameter is required'\n",
    "        }), 400\n",
    "        \n",
    "    if not type_dataset:\n",
    "        return jsonify({\n",
    "            'message': 'Type dataset parameter is required'\n",
    "        }), 400\n",
    "\n",
    "    if type_dataset == \"1\":\n",
    "        search_engine.load_model(\"trec-tot\")\n",
    "    elif type_dataset == \"2\":\n",
    "        search_engine.load_model(\"webis-touche2020\")\n",
    "    else:\n",
    "        return jsonify({\n",
    "            'message': \"Type dataset not valid\"\n",
    "        }), 400\n",
    "        \n",
    "    ranked_indices, scores, corrected_query = search_engine.search(query, weights)\n",
    "    \n",
    "    results = []\n",
    "    for index in ranked_indices[:10]:\n",
    "        doc = search_engine.documents[index]\n",
    "        results.append({\n",
    "            'title': doc['page_title'],\n",
    "            'wikidata_classes': doc['wikidata_classes'],\n",
    "            'text_snippet': ' '.join(doc['text'].split()[:30]),\n",
    "            'similarity_score': f'{scores[index]:.4f}'\n",
    "        })\n",
    "\n",
    "    response = {\n",
    "        'corrected_query': corrected_query if corrected_query.lower() != query.lower() else None,\n",
    "        'results': results\n",
    "    }\n",
    "    return jsonify(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fab291de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with watchdog (windowsapi)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
