{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "168b548c",
   "metadata": {},
   "source": [
    "# Notebook: Search Engine with Python, NLTK, and Scikit-learn\n",
    "# Author: [Yasser Barghouth]\n",
    "\n",
    "## Description: This notebook demonstrates the creation of a simple search engine using a service-oriented approach.\n",
    "##              It utilizes NLTK for text preprocessing and Scikit-learn for vectorization and search functionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecde78b9",
   "metadata": {},
   "source": [
    "# IR Search Engine\n",
    "\n",
    "This notebook implements an Information Retrieval (IR) search engine using various datasets. It includes preprocessing, spell correction, vectorization, and a search mechanism. The application is built using Flask and SQLAlchemy.\n",
    "\n",
    "## Components\n",
    "1. Preprocessor\n",
    "2. SpellCorrector\n",
    "3. Vectorizer\n",
    "4. SearchEngine\n",
    "5. Flask API\n",
    "\n",
    "## Datasets\n",
    "- TREC-TOT\n",
    "- Webis-Touche\n",
    "\n",
    "## Usage\n",
    "The search engine supports indexing documents from datasets, processing queries, and returning relevant documents based on cosine similarity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4a9440",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4111b512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import joblib\n",
    "import re\n",
    "import os\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_sqlalchemy import SQLAlchemy\n",
    "from sqlalchemy import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab10e44",
   "metadata": {},
   "source": [
    "## Preprocessor Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0f5d7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessor Class: Tokenization, stopword removal, lemmatization\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def get_wordnet_pos(self, tag):\n",
    "        tag_dict = {\n",
    "            \"J\": wordnet.ADJ,\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"R\": wordnet.ADV\n",
    "        }\n",
    "        return tag_dict.get(tag[0].upper(), wordnet.NOUN)\n",
    "\n",
    "    def lemmatization(self, tagged_doc_text):\n",
    "        return [self.lemmatizer.lemmatize(word, pos=self.get_wordnet_pos(tag)) for word, tag in tagged_doc_text]\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tokens = [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens if token.translate(str.maketrans('', '', string.punctuation))]\n",
    "        tokens = [word for word in tokens if word not in self.stop_words]\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        tokens = self.lemmatization(tagged_tokens)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4684d8",
   "metadata": {},
   "source": [
    "## SpellCorrector Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8300fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpellCorrector Class: Corrects misspelled words in a query\n",
    "class SpellCorrector:\n",
    "    def __init__(self):\n",
    "        self.spell_checker = SpellChecker()\n",
    "\n",
    "    def correct_sentence_spelling(self, query):\n",
    "        query_tokens = word_tokenize(query.lower())\n",
    "        misspelled = self.spell_checker.unknown(query_tokens)\n",
    "        corrected_tokens = [self.spell_checker.correction(token) if token in misspelled else token for token in query_tokens]\n",
    "        return ' '.join(corrected_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc47e5d",
   "metadata": {},
   "source": [
    "## Vectorizer Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d143ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer Class: TF-IDF Vectorization\n",
    "class Vectorizer:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def fit_transform(self, documents):\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(documents)\n",
    "        return self.tfidf_matrix\n",
    "\n",
    "    def transform(self, document):\n",
    "        return self.vectorizer.transform([document])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9ac8c1",
   "metadata": {},
   "source": [
    "## SearchEngine Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ba9a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SearchEngine Class: Main search engine logic\n",
    "class SearchEngine:\n",
    "    def __init__(self, preprocessor, spell_corrector, document_service, model_service):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.spell_corrector = spell_corrector\n",
    "        self.document_service = document_service\n",
    "        self.model_service = model_service\n",
    "        self.vectorizers = {}\n",
    "        self.tfidf_matrices = {}\n",
    "        self.documents = []\n",
    "        self.elements = []\n",
    "\n",
    "    def index_documents(self, dataset_name):\n",
    "        self.documents = self.document_service.get_documents(dataset_name)\n",
    "        self.elements = self.document_service.get_columns(dataset_name)\n",
    "        for element in self.elements:\n",
    "            try:\n",
    "                processed_docs = [' '.join(self.preprocessor.preprocess(doc[element])) for doc in self.documents]\n",
    "                vectorizer = Vectorizer()\n",
    "                self.vectorizers[element] = vectorizer\n",
    "                self.tfidf_matrices[element] = vectorizer.fit_transform(processed_docs)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred during vectorization of {element}:\", e)\n",
    "    \n",
    "    def save_model(self, name):\n",
    "        try:\n",
    "            self.model_service.save_vectorizers(self.vectorizers, name)\n",
    "            self.model_service.save_tfidf_matrices(self.tfidf_matrices, name)\n",
    "            return jsonify({'message': f'{name} vectorizers and tf-idf matrixes saved successfully'}), 200\n",
    "        \n",
    "        except ValueError as e:\n",
    "            return jsonify({'message': str(e)}), 400    \n",
    "\n",
    "    def load_model(self, name):\n",
    "        try:\n",
    "            self.vectorizers = self.model_service.load_vectorizers(name)\n",
    "            self.tfidf_matrices = self.model_service.load_tfidf_matrices(name)\n",
    "            return jsonify({'message': f'{name} vectorizers and tf-idf matrices loaded successfully'}), 200\n",
    "        except ValueError as e:\n",
    "            return jsonify({'message': str(e)}), 400\n",
    "\n",
    "    \n",
    "    def search(self, query, dataset_name, weights=None):\n",
    "        if not self.vectorizers or not self.tfidf_matrices:\n",
    "            return [], [], query, \"Model not loaded\"\n",
    "        \n",
    "        if weights is None:\n",
    "            if dataset_name == \"trec-tot\":\n",
    "                weights = [0.5, 1.5, 0.3, 0.1, 0.1]\n",
    "            elif dataset_name == \"webis-touche\":\n",
    "                weights = [0.5, 0.3]\n",
    "\n",
    "        corrected_query = self.spell_corrector.correct_sentence_spelling(query)\n",
    "        query_processed = self.preprocessor.preprocess(corrected_query)\n",
    "        if not query_processed:\n",
    "            return [], [], corrected_query, \"Query could not be processed\"\n",
    "        \n",
    "        query = ' '.join(query_processed)\n",
    "        \n",
    "        document_count = self.document_service.get_document_count(dataset_name)        \n",
    "        scores = np.zeros(len(document_count))\n",
    "\n",
    "        self.elements = self.document_service.get_columns(dataset_name)\n",
    "        for element, weight in zip(self.elements, weights):\n",
    "            try:\n",
    "                query_vector = self.vectorizers[element].transform(query)\n",
    "                if query_vector.shape[1] == 0:\n",
    "                    continue\n",
    "                cosine_similarities = cosine_similarity(query_vector, self.tfidf_matrices[element]).flatten()\n",
    "                scores += weight * cosine_similarities\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred during searching in {element}:\", e)\n",
    "\n",
    "        ranked_indices = np.argsort(scores)[::-1]\n",
    "        return ranked_indices, scores, corrected_query, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77b6f9a",
   "metadata": {},
   "source": [
    "## Database Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770d7c08",
   "metadata": {},
   "source": [
    "### TrecTot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3b3e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SQLAlchemy instance\n",
    "db = SQLAlchemy()\n",
    "\n",
    "# Database Model for datasets\n",
    "class TrecTotDocument(db.Model):\n",
    "    id = db.Column(db.Integer, primary_key=True)\n",
    "    page_title = db.Column(db.String(255))\n",
    "    wikidata_classes = db.Column(db.String(255))\n",
    "    text = db.Column(db.Text)\n",
    "    sections = db.Column(db.Text)\n",
    "    infoboxes = db.Column(db.Text)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'page_title': self.page_title,\n",
    "            'wikidata_classes': self.wikidata_classes,\n",
    "            'text': self.text,\n",
    "            'sections': self.sections,\n",
    "            'infoboxes': self.infoboxes,\n",
    "        }\n",
    "        \n",
    "    def get_columns(self, exclude_id):\n",
    "        columns = self.__table__.columns\n",
    "        if exclude_id:\n",
    "            return [column.name for column in columns if column.name != 'id']\n",
    "        else:\n",
    "            return [column.name for column in columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7662f10a",
   "metadata": {},
   "source": [
    "### WebisTouche model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d8c58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Model for datasets\n",
    "class WebisToucheDocument(db.Model):\n",
    "    id = db.Column(db.Integer, primary_key=True)\n",
    "    title = db.Column(db.String(255))\n",
    "    text = db.Column(db.Text)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'title': self.title,\n",
    "            'text': self.text,\n",
    "        }\n",
    "        \n",
    "    def get_columns(self, exclude_id):\n",
    "        columns = self.__table__.columns\n",
    "        if exclude_id:\n",
    "            return [column.name for column in columns if column.name != 'id']\n",
    "        else:\n",
    "            return [column.name for column in columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513c286b",
   "metadata": {},
   "source": [
    "### Document Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e54a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DocumentService class: Interacts with the database to manage documents\n",
    "class DocumentService:\n",
    "    def __init__(self, db):\n",
    "        self.db = db\n",
    "\n",
    "    def save_document(self, doc_data, dataset_name):\n",
    "        if dataset_name == \"trec-tot\":\n",
    "            doc = TrecTotDocument(\n",
    "                page_title=doc_data['page_title'],\n",
    "                wikidata_classes=doc_data['wikidata_classes'],\n",
    "                text=doc_data['text'],\n",
    "                sections=doc_data['sections'],\n",
    "                infoboxes=doc_data['infoboxes']\n",
    "            )\n",
    "        elif dataset_name == \"webis-touche\":\n",
    "            doc = WebisToucheDocument(\n",
    "                title=doc_data['title'],\n",
    "                text=doc_data['text']\n",
    "            )\n",
    "        self.db.session.add(doc)\n",
    "        self.db.session.commit()\n",
    "\n",
    "    def get_documents(self, dataset_name):\n",
    "        if dataset_name == \"trec-tot\":\n",
    "            return TrecTotDocument.query.limit(100).all()\n",
    "        elif dataset_name == \"webis-touche\":\n",
    "            return WebisToucheDocument.query.limit(100).all()\n",
    "\n",
    "    def get_columns(self, dataset_name, exclude_id):\n",
    "        if dataset_name == \"trec-tot\":\n",
    "            return TrecTotDocument.get_columns(exclude_id)\n",
    "        elif dataset_name == \"webis-touche\":\n",
    "            return WebisToucheDocument.get_columns(exclude_id)\n",
    "        \n",
    "    def check_data_exists(self, dataset_name):\n",
    "        table = self.db.metadata.tables.get(dataset_name)\n",
    "        if table is not None:\n",
    "            count = self.db.session.query(table).count()\n",
    "            return count > 0\n",
    "        \n",
    "    def check_table_exists(self, dataset_name):\n",
    "        inspector = inspect(self.db.engine)\n",
    "        return dataset_name in inspector.get_table_names()\n",
    "    \n",
    "    def get_documents_by_indices(self, indices, dataset_name):\n",
    "        if dataset_name == \"trec-tot\":\n",
    "            return TrecTotDocument.query.filter(TrecTotDocument.id.in_(indices)).all()\n",
    "        elif dataset_name == \"webis-touche\":\n",
    "            return WebisToucheDocument.query.filter(WebisToucheDocument.id.in_(indices)).all()\n",
    "        \n",
    "    def get_document_count(self, dataset_name):\n",
    "        if dataset_name == \"trec-tot\":\n",
    "            return TrecTotDocument.query.count()\n",
    "        elif dataset_name == \"webis-touche\":\n",
    "            return WebisToucheDocument.query.count()\n",
    "\n",
    "def clean_value(value):\n",
    "    text = re.sub(r\"'''''(.*?)'''''\", r\"\\1\", value)\n",
    "    text = re.sub(r\"'''(.*?)'''\", r\"\\1\", text)\n",
    "    text = re.sub(r\"''(.*?)''\", r\"\\1\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9248e4c3",
   "metadata": {},
   "source": [
    "## Model Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cf21d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelService class: Manages saving/loading of models\n",
    "class ModelService:\n",
    "    def __init__(self, model_directory):\n",
    "        self.model_directory = model_directory\n",
    "    def save_vectorizers(self, vectorizers, name):\n",
    "        try:\n",
    "            joblib.dump(vectorizers, f'{self.model_directory}/{name}_vectorizers.pkl')\n",
    "        except ValueError as e:\n",
    "            return f'{name}_vectorizers.pkl can\\'t saved'\n",
    "\n",
    "    def save_tfidf_matrices(self, tfidf_matrices, name):\n",
    "        try:\n",
    "            joblib.dump(tfidf_matrices, f'{self.model_directory}/{name}_tfidf_matrices.pkl')\n",
    "        except ValueError as e:\n",
    "            return f'{name}_vectorizers.pkl can\\'t saved'\n",
    "\n",
    "    def load_vectorizers(self, name):\n",
    "        try:\n",
    "            return joblib.load(f'{self.model_directory}/{name}_vectorizers.pkl')\n",
    "        except ValueError as e:\n",
    "            return f'{name}_vectorizers.pkl can\\'t loaded' \n",
    "\n",
    "    def load_tfidf_matrices(self, name):\n",
    "        try:\n",
    "            return joblib.load(f'{self.model_directory}/{name}_tfidf_matrices.pkl')\n",
    "        except ValueError as e:\n",
    "            return f'{name}_tfidf_matrices.pkl can\\'t loaded' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3084b1",
   "metadata": {},
   "source": [
    "## Data Loading Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e8fbc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset(dataset_name):\n",
    "    if dataset_name == \"trec-tot\":\n",
    "        dataset = ir_datasets.load(\"trec-tot/2023/train\")\n",
    "    elif dataset_name == \"webis-touche\":\n",
    "        dataset = ir_datasets.load(\"beir/webis-touche2020\")\n",
    "    else:\n",
    "        raise ValueError('Invalid dataset name')\n",
    "\n",
    "    with app.app_context():\n",
    "        if not document_service.check_table_exists(dataset_name):\n",
    "            db.create_all()   # Create tables if they don't exist\n",
    "\n",
    "        if not document_service.check_data_exists(dataset_name):\n",
    "            for i, doc in enumerate(dataset.docs_iter()):\n",
    "                if dataset_name == \"trec-tot\":\n",
    "                    doc_data = {\n",
    "                        \"page_title\": doc.page_title,\n",
    "                        \"wikidata_classes\": doc.wikidata_classes[0][1],\n",
    "                        \"text\": doc.text,\n",
    "                        \"sections\": \"\",\n",
    "                        \"infoboxes\": \"\",\n",
    "                    }\n",
    "\n",
    "                    for section_name, section_text in doc.sections.items():\n",
    "                        doc_data[\"sections\"] += f\"\\n {section_text}\"\n",
    "\n",
    "                    for infobox in doc.infoboxes:\n",
    "                        for key, value in infobox['params'].items():\n",
    "                            cleaned_value = clean_value(value)\n",
    "                            doc_data[\"infoboxes\"] += f\"\\n {cleaned_value}\"\n",
    "                elif dataset_name == \"webis-touche\":\n",
    "                    doc_data = {\n",
    "                        \"title\": doc.title,\n",
    "                        \"text\": doc.text,\n",
    "                    }\n",
    "                    \n",
    "                document_service.save_document(doc_data, dataset_name)\n",
    "    \n",
    "        else:\n",
    "                raise ValueError(f'{dataset_name} dataset already exists')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684fd7f9",
   "metadata": {},
   "source": [
    "## Flask App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a02f626c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [01/Jun/2024 14:16:04] \"POST /load_index_save_dataset HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# Flask app setup\n",
    "app = Flask(__name__)\n",
    "app.config['SQLALCHEMY_DATABASE_URI'] = 'mysql://root:@localhost/ir_search_engine'\n",
    "db.init_app(app)\n",
    "\n",
    "# Instantiate services\n",
    "document_service = DocumentService(db)\n",
    "preprocessor = Preprocessor()\n",
    "spell_corrector = SpellCorrector()\n",
    "model_service = ModelService(model_directory='models')\n",
    "search_engine = SearchEngine(preprocessor, spell_corrector, document_service, model_service)\n",
    "\n",
    "@app.route('/load_index_save_dataset', methods=['POST'])\n",
    "def load_index_save_dataset_route():\n",
    "     # Check if request has JSON data\n",
    "    if not request.is_json:\n",
    "        return jsonify({'message': 'Request must be JSON'}), 400\n",
    "\n",
    "    data = request.get_json()\n",
    "    \n",
    "    dataset_name = request.json.get('dataset_name', '').strip()\n",
    "    if not dataset_name:\n",
    "        return jsonify({'message': 'Dataset name is required'}), 400\n",
    "    try:\n",
    "        load_dataset(dataset_name)\n",
    "        # search_engine.index_documents(dataset_name)\n",
    "        # search_engine.save_model(dataset_name)\n",
    "        return jsonify({'message': f'{dataset_name} dataset loaded and indexed successfully'}), 200\n",
    "    except ValueError as e:\n",
    "        return jsonify({'message': str(e)}), 400\n",
    "\n",
    "@app.route('/search', methods=['GET'])\n",
    "def search():\n",
    "    query = request.args.get('query', '').strip()\n",
    "    type_dataset = request.args.get('type_dataset', '').strip()\n",
    "    weights = request.args.getlist('weights', type=float)\n",
    "    page = request.args.get('page', default=1, type=int)\n",
    "    limit = request.args.get('limit', default=10, type=int)\n",
    "    \n",
    "    if not query:\n",
    "        return jsonify({\n",
    "            'message': 'Query parameter is required'\n",
    "        }), 400\n",
    "        \n",
    "    if not type_dataset:\n",
    "        return jsonify({\n",
    "            'message': 'Type dataset parameter is required'\n",
    "        }), 400\n",
    "\n",
    "    if type_dataset not in ['1', '2']:\n",
    "        return jsonify({'message': 'Type dataset not valid'}), 400\n",
    "    \n",
    "    if type_dataset == \"1\":\n",
    "        dataset_name = \"trec-tot\"\n",
    "    elif type_dataset == \"2\":\n",
    "        dataset_name = \"webis-touche\"\n",
    "    \n",
    "    if not document_service.check_table_exists(dataset_name) or not document_service.check_data_exists(dataset_name):\n",
    "        return jsonify({\n",
    "            'message': f'{dataset_name} Dataset is not loaded. Please load the dataset first.'\n",
    "        }), 400\n",
    "\n",
    "    model_loaded = search_engine.load_model(dataset_name)\n",
    "    if not model_loaded:\n",
    "        return jsonify({'message': 'Model is not loaded. Please index the documents first.'}), 400\n",
    "\n",
    "    ranked_indices, scores, corrected_query, error = search_engine.search(query, dataset_name, weights if weights else None)\n",
    "    if error:\n",
    "        return jsonify({'message': error}), 400\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    start_index = (page - 1) * limit\n",
    "    end_index = start_index + limit\n",
    "    top_indices = ranked_indices[start_index:end_index]\n",
    "    docs = document_service.get_documents_by_indices(top_indices, dataset_name)\n",
    "    \n",
    "    if dataset_name == \"trec-tot\":\n",
    "        for index in top_indices:\n",
    "            doc = docs[index].to_dict()\n",
    "            results.append({\n",
    "                'title': doc['page_title'],\n",
    "                'wikidata_classes': doc['wikidata_classes'],\n",
    "                'text_snippet': ' '.join(doc['text'].split()[:30]),\n",
    "                'similarity_score': f'{scores[index]:.4f}'\n",
    "            })\n",
    "    else:\n",
    "        for index in ranked_indices[:10]:\n",
    "            results.append({\n",
    "                'title': doc['title'],\n",
    "                'text_snippet': ' '.join(doc['text'].split()[:30]),\n",
    "                'similarity_score': f'{scores[index]:.4f}'\n",
    "            })\n",
    "            \n",
    "\n",
    "    response = {\n",
    "        'corrected_query': corrected_query if corrected_query.lower() != query.lower() else None,\n",
    "        'results': results\n",
    "    }\n",
    "    \n",
    "    return jsonify(response)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
